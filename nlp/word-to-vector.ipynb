{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''\n",
    "About Wikipedia\n",
    "This is the front page of the Simple English Wikipedia. Wikipedias are places where people work together to write encyclopedias in different languages. We use Simple English words and grammar here. The Simple English Wikipedia is for everyone, such as children and adults who are learning English. There are 267,477 articles on the Simple English Wikipedia. All of the pages are free to use. They have all been published under both the Creative Commons Attribution/Share-Alike License 4.0 International License and the GNU Free Documentation License. You can help here! You may change these pages and make new pages. Read the help pages and other good pages to learn how to write pages here. If you need help, you may ask questions at Simple talk.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfd85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # Ensure punkt_tab is downloaded\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43927f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336cde3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in sentences:\n",
    "  sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "  corpus.append(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c17378",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500950ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2886309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "for i in corpus:\n",
    "  words = nltk.word_tokenize(i)\n",
    "  for word in words:\n",
    "    if word not in set(stopwords.words('english')):\n",
    "      print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0bdea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "for i in corpus:\n",
    "  words = nltk.word_tokenize(i)\n",
    "  for word in words:\n",
    "    if word not in set(stopwords.words('english')):\n",
    "      print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c55522aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Stop word and lemmatization\n",
    "\n",
    "corpus = []\n",
    "for i in sentences:\n",
    "  review = re.sub('[^a-zA-Z]', ' ', i)\n",
    "  review = review.lower()\n",
    "  review = nltk.word_tokenize(review)\n",
    "  review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer( binary=True, ngram_range=(1, 3))\n",
    "\n",
    "X = cv.fit_transform(corpus)\n",
    "print(cv.vocabulary_)\n",
    "print(corpus[0])\n",
    "print(X[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "580e909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(ngram_range=(1, 3), max_features=3)\n",
    "X = tv.fit_transform(corpus)\n",
    "print(tv.vocabulary_)\n",
    "print(corpus[0])\n",
    "print(X[0].toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
